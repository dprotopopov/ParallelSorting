char *title = "odd-even sort";
char *description = "Сортировка четно-нечетными перестановками (odd-even sort)";
/*
Для каждой итерации алгоритма операции сравнения-обмена для всех пар элементов независимы и
выполняются одновременно. Рассмотрим случай, когда число процессоров равно числу элементов, т.е. p=n -
число процессоров (сортируемых элементов). Предположим, что вычислительная система имеет топологию
кольца. Пусть элементы ai (i = 1, .. , n), первоначально расположены на процессорах pi (i = 1, ... , n). В нечетной
итерации каждый процессор с нечетным номером производит сравнение-обмен своего элемента с элементом,
находящимся на процессоре-соседе справа. Аналогично в течение четной итерации каждый процессор с четным
номером производит сравнение-обмен своего элемента с элементом правого соседа.
На каждой итерации алгоритма нечетные и четные процессоры выполняют шаг сравнения-обмена с их
правыми соседями за время Q(1). Общее количество таких итераций – n; поэтому время выполнения
параллельной сортировки – Q(n).
Когда число процессоров p меньше числа элементов n, то каждый из процессов получает свой блок
данных n/p и сортирует его за время Q((n/p)·log(n/p)). Затем процессоры проходят p итераций (р/2 и чётных, и
нечётных) и делают сравнивания-разбиения: смежные процессоры передают друг другу свои данные, а
внутренне их сортируют (на каждой паре процессоров получаем одинаковые массивы). Затем удвоенный
массив делится на 2 части; левый процессор обрабатывает далее только левую часть (с меньшими значениями
данных), а правый – только правую (с большими значениями данных). Получаем отсортированный массив
после p итераций, выполняя в каждой такие шаги:
23)узлы считывают входные данные из хранилища;
24)хост распределяет данные между клонами;
25)клоны сортируют свою часть данных;
26)клоны обмениваются частями, при этом объединяя четные и нечетные части;
27)если количество перестановок равно количеству клонов, то алгоритм завершен;
28)хост формирует отсортированный массив.
*/

#include <stdio.h>
#include <stdlib.h> 
#include <mpi.h> 

#ifndef max
	#define max( a, b ) ( ((a) > (b)) ? (a) : (b) )
#endif

#ifndef min
	#define min( a, b ) ( ((a) < (b)) ? (a) : (b) )
#endif

/* Фунция сравнения двух элементов для определения порядка сортировки */ 
/* Возвращает <0 если e1 предшедствует e2, 0 если елементы равны и >0 если e2 предшедствует e1 */
int asc_order(const void *e1, const void *e2) 
{ 
	return (*((int *)e1) - *((int *)e2)); 
}
int desc_order(const void *e1, const void *e2) 
{ 
	return (*((int *)e2) - *((int *)e1)); 
}

main(int argc, char *argv[]) 
{ 
	int n;         /* Размер сортируемого массива */ 
	int nrank;      /* Общее количество процессов */ 
	int myrank;    /* Номер текущего процесса */ 
	int maxnlocal;    /* Максимальное количество элементов, хранимых локально */ 
	int nlocal;    /* Количество элементов, хранимых локально */ 
	int nremote;    /* Количество элементов, полученных при операции обмена от соседа */ 
	int *elements;   /* Массив элементов, хранимые локально */ 
	int oddrank;   /* Номер соседа при нечётной фазе обмена */ 
	int evenrank;  /* Номер соседа при чётной фазе обмена */ 
	int direction; /* Порядок сортировки 1 - по возрастанию, -1 по убыванию */
	int i, j; 
	int start, end;
	MPI_Status status; 

	/* Иницилизация MPI */ 
	MPI_Init(&argc, &argv); 
	MPI_Comm_size(MPI_COMM_WORLD, &nrank); 
	MPI_Comm_rank(MPI_COMM_WORLD, &myrank); 

	n = atoi(argv[1]); 
	direction = 1;

	if (myrank == 0) {
		printf("Title :\t%s\n", title);
		printf("Description :\t%s\n", description);
		printf("Number of processes :\t%d\n", nrank);
		printf("Array size :\t%d\n", n);
	} 

	maxnlocal = (int)((n+nrank-1)/nrank); /* Вычисляем максимальное количество элементов хранимых одним процессом */ 

	/* Выделяем память под хранение данных */ 
	if (myrank == 0) {
		elements  = (int *)malloc(max(n*sizeof(int),2*maxnlocal*sizeof(int))); 
	} else {
		elements  = (int *)malloc(2*maxnlocal*sizeof(int)); 
	}
	
	if (elements) {
		/* Заполняем массив псевдо-случайными числами */ 
		/* Операция выполняетя только на ведущем процессе */ 
		if (myrank == 0) {
			for (i=0; i<n; i++) {
				elements[i] = random(); 
			}
		}
	} else {
		printf("Prosess %d malloc error\n", myrank);
	}

	/* Распределям данные между процессами */
	/* Количество данных полученных одним прцессом может различатся */
	/* поскольку размер исходных данных может быть не кратным количеиву процессов */
	if (myrank == 0) {
		nlocal = n;
		for(i=nrank; i>1 ; i--) {
			nremote = (int)nlocal/i;
			nlocal -= nremote;
			MPI_Send(&nremote, 1, MPI_INT, i-1, 1, MPI_COMM_WORLD); /* Уведомляем о количестве передаваемых данных */
			if(nremote > 0) MPI_Send(&elements[nlocal], nremote, MPI_INT, i-1, 1, MPI_COMM_WORLD); /* Отправляем данные */
		}
	} else {
		MPI_Recv(&nlocal, 1, MPI_INT, 0, 1, MPI_COMM_WORLD, &status); 
		if(nlocal > 0) MPI_Recv(elements, maxnlocal, MPI_INT, 0, 1, MPI_COMM_WORLD, &status); 
	}

	printf("Process %d local array size : %d\n", myrank, nlocal);

	/* Определяем номера процессов, используемых при операциях обмена с соседом */ 
	if (myrank%2 == 0) { 
		oddrank  = myrank-1; 
		evenrank = myrank+1; 
	} 
	else { 
		oddrank  = myrank+1; 
		evenrank = myrank-1; 
	} 

	/* Поскольку мы не можем гарантировать чётное количество процессов, то организуем процессы в линейную схему */ 
	if (oddrank == -1 || oddrank == nrank) 
		oddrank = MPI_PROC_NULL; 
	if (evenrank == -1 || evenrank == nrank) 
		evenrank = MPI_PROC_NULL; 

	/* Выполняем предварительный цикл алгоритма */ 

	qsort(elements, nlocal, sizeof(int), ((direction>0)?asc_order:desc_order)); 

	/* Выполняем основной цикл алгоритма */ 
	for (i=0; i<nrank; i++) { 
		int neigborrank = (i%2 == 1)?oddrank:evenrank;
		nremote = 0;
		/* Обмениваемся информацией о количестве данных */
		MPI_Sendrecv(&nlocal, 1, MPI_INT, neigborrank, 1, &nremote, 1, MPI_INT, neigborrank, 1, MPI_COMM_WORLD, &status); 
		/* Обмениваемся данными - полученные данные загружаем в конец локального массива */
		MPI_Sendrecv(elements, maxnlocal, MPI_INT, neigborrank, 1, &elements[nlocal], maxnlocal, MPI_INT, neigborrank, 1, MPI_COMM_WORLD, &status); 
		
		printf("Process %d iteraction %d recieved %d items from %d\n", myrank, i, nremote, neigborrank);

		/* Сортируем локальные и полученные элементы */
		/* Для упрощения программы используем функцию qsort */
		/* хотя можно использовать алгоритм слияния двух уже отсортированных массивов */
		qsort(elements, nlocal+nremote, sizeof(int), ((direction>0)?asc_order:desc_order)); 

		/* Оставляем у себя только левую или правую часть в зависимости от чётности шага и номера процесса */
		if ( (myrank < neigborrank) && (myrank < nrank-1) ) { 
			/* Был обмен с процессом справа */ 
			/* Данные лежат в начале объединёного массива - ничего делать не надо */
		} else if ( (myrank > neigborrank) && (myrank > 0) ) {
			/* Был обмен с процессом слева */ 
			/* Данные лежат в конце объединёного массива - перемещаем их в начало массива */
			for(j=0;j<nlocal; j++) elements[j] = elements[j+nremote];
		}
	} 

	/* Собираем данные с ведомых процессов */
	if (myrank == 0) {
		for(i=1; i<nrank ; i++) {
			nremote = 0;
			MPI_Recv(&nremote, 1, MPI_INT, i, 1, MPI_COMM_WORLD, &status); 
			if(nremote > 0) MPI_Recv(&elements[nlocal], nremote, MPI_INT, i, 1, MPI_COMM_WORLD, &status);
			nlocal += nremote;
		}
	} else {
		MPI_Send(&nlocal, 1, MPI_INT, 0, 1, MPI_COMM_WORLD); /* Уведомляем о количестве передаваемых данных */
		if(nlocal > 0) MPI_Send(elements, nlocal, MPI_INT, 0, 1, MPI_COMM_WORLD); /* Отправляем данные */			 
	}

	/* Проверяем и выводим результаты */
	if (myrank == 0) {
		int check = 1;
		for(i=1; i<n && check ;i++) {
			check = (direction*asc_order(&elements[i-1],&elements[i]) <= 0)?1:0;
		}
		
		printf("Check :\t%s\n", (check?"ok":"fail"));

		for(i=0; i<n && i<20;i++) {	printf("%d,",elements[i]); } printf("...\n");
	} 

	/* Освобождаем ранее выделенные ресурсы */
	free(elements); 

	MPI_Finalize(); 

	exit(0);
} 


